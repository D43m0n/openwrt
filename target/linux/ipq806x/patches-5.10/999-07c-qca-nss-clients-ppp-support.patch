--- a/drivers/net/ppp/ppp_generic.c
+++ b/drivers/net/ppp/ppp_generic.c
*************** void ppp_release_channels(struct ppp_cha
*** 3706,3711 ****
--- 3706,3727 ----
  }
  EXPORT_SYMBOL(ppp_release_channels);
  
+ /* Return the PPP net device index */
+ int ppp_dev_index(struct ppp_channel *chan)
+ {
+        struct channel *pch = chan->ppp;
+        int ifindex = 0;
+ 
+        if (pch) {
+ 	       read_lock_bh(&pch->upl);
+ 	       if (pch->ppp && pch->ppp->dev)
+ 		       ifindex = pch->ppp->dev->ifindex;
+ 	       read_unlock_bh(&pch->upl);
+        }
+        return ifindex;
+ }
+ EXPORT_SYMBOL(ppp_dev_index);
+ 
  /* Module/initialization stuff */
  
  module_init(ppp_init);
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -982,6 +982,20 @@ struct dev_ifalias {
 struct devlink;
 struct tlsdev_ops;
 
+struct flow_offload;
+struct flow_offload_hw_path;
+
+enum flow_offload_type {
+        FLOW_OFFLOAD_ADD        = 0,
+        FLOW_OFFLOAD_DEL,
+};
+
+enum nss_flow_offload_type {
+        NF_FLOW_OFFLOAD_UNSPEC = 0,
+        NF_FLOW_OFFLOAD_ROUTE,
+};
+
+
 struct netdev_name_node {
 	struct hlist_node hlist;
 	struct list_head list;
@@ -1498,6 +1512,12 @@ struct net_device_ops {
 	int			(*ndo_bridge_dellink)(struct net_device *dev,
 						      struct nlmsghdr *nlh,
 						      u16 flags);
+	int                     (*ndo_flow_offload_check)(struct flow_offload_hw_path *path);
+        int                     (*ndo_flow_offload)(enum nss_flow_offload_type type,
+                                                    struct flow_offload *flow,
+                                                    struct flow_offload_hw_path *src,
+                                                    struct flow_offload_hw_path *dest);
+
 	int			(*ndo_change_carrier)(struct net_device *dev,
 						      bool new_carrier);
 	int			(*ndo_get_phys_port_id)(struct net_device *dev,
--- a/include/linux/netfilter/nf_conntrack_proto_gre.h
+++ b/include/linux/netfilter/nf_conntrack_proto_gre.h
@@ -31,4 +31,36 @@ void nf_ct_gre_keymap_destroy(struct nf_
 
 bool gre_pkt_to_tuple(const struct sk_buff *skb, unsigned int dataoff,
 		      struct net *net, struct nf_conntrack_tuple *tuple);
+
+/* QCA NSS ECM Support - Start */
+/* GRE is a mess: Four different standards */
+struct gre_hdr {
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+        __u16   rec:3,
+                srr:1,
+                seq:1,
+                key:1,
+                routing:1,
+                csum:1,
+                version:3,
+                reserved:4,
+                ack:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+        __u16   csum:1,
+                routing:1,
+                key:1,
+                seq:1,
+                srr:1,
+                rec:3,
+                ack:1,
+                reserved:4,
+                version:3;
+#else
+#error "Adjust your <asm/byteorder.h> defines"
+#endif
+        __be16  protocol;
+};
+/* QCA NSS ECM Support - End */
+
+
 #endif /* _CONNTRACK_PROTO_GRE_H */
--- a/net/ipv4/ip_gre.c
+++ b/net/ipv4/ip_gre.c
@@ -1325,6 +1325,7 @@ static void ipgre_tap_setup(struct net_d
 	dev->netdev_ops	= &gre_tap_netdev_ops;
 	dev->priv_flags &= ~IFF_TX_SKB_SHARING;
 	dev->priv_flags	|= IFF_LIVE_ADDR_CHANGE;
+	dev->priv_flags_qca_ecm |= IFF_QCA_ECM_GRE_V4_TAP; /* QCA NSS ECM Support */
 	ip_tunnel_setup(dev, gre_tap_net_id);
 }
 
--- a/net/ipv6/ip6_gre.c
+++ b/net/ipv6/ip6_gre.c
@@ -1921,6 +1921,7 @@ static void ip6gre_tap_setup(struct net_
 
 	dev->priv_flags &= ~IFF_TX_SKB_SHARING;
 	dev->priv_flags |= IFF_LIVE_ADDR_CHANGE;
+	dev->priv_flags_qca_ecm |= IFF_QCA_ECM_GRE_V6_TAP; /* QCA NSS ECM Support */
 	netif_keep_dst(dev);
 }
 
--- a/include/net/netfilter/nf_flow_table.h
+++ b/include/net/netfilter/nf_flow_table.h
@@ -157,10 +157,14 @@ enum nf_flow_flags {
 	NF_FLOW_HW_PENDING,
 };
 
-enum flow_offload_type {
-	NF_FLOW_OFFLOAD_UNSPEC	= 0,
-	NF_FLOW_OFFLOAD_ROUTE,
-};
+
+#define FLOW_OFFLOAD_SNAT       0x1
+#define FLOW_OFFLOAD_DNAT       0x2
+#define FLOW_OFFLOAD_DYING      0x4
+#define FLOW_OFFLOAD_TEARDOWN   0x8
+#define FLOW_OFFLOAD_HW         0x10
+#define FLOW_OFFLOAD_KEEP       0x20
+
 
 struct flow_offload {
 	struct flow_offload_tuple_rhash		tuplehash[FLOW_OFFLOAD_DIR_MAX];
@@ -171,6 +175,25 @@ struct flow_offload {
 	struct rcu_head				rcu_head;
 };
 
+
+#define FLOW_OFFLOAD_PATH_ETHERNET      BIT(0)
+#define FLOW_OFFLOAD_PATH_VLAN          BIT(1)
+#define FLOW_OFFLOAD_PATH_PPPOE         BIT(2)
+#define FLOW_OFFLOAD_PATH_DSA           BIT(3)
+
+struct flow_offload_hw_path {
+        struct net_device *dev;
+        u32 flags;
+
+        u8 eth_src[ETH_ALEN];
+        u8 eth_dest[ETH_ALEN];
+        u16 vlan_proto;
+        u16 vlan_id;
+        u16 pppoe_sid;
+        u16 dsa_port;
+};
+
+
 #define NF_FLOW_TIMEOUT (30 * HZ)
 #define nf_flowtable_time_stamp	(u32)jiffies
 
--- a/drivers/net/ppp/pppoe.c
+++ b/drivers/net/ppp/pppoe.c
@@ -73,6 +73,12 @@
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+#include <linux/netfilter.h>
+#include <net/netfilter/nf_flow_table.h>
+#endif
+
+
 #include <linux/nsproxy.h>
 #include <net/net_namespace.h>
 #include <net/netns/generic.h>
--- a/drivers/net/bonding/bond_main.c
+++ b/drivers/net/bonding/bond_main.c
@@ -202,6 +202,7 @@ atomic_t netpoll_block_tx = ATOMIC_INIT(
 #endif
 
 unsigned int bond_net_id __read_mostly;
+static unsigned long bond_id_mask = 0xFFFFFFF0; /* QCA NSS ECM Support */
 
 static const struct flow_dissector_key flow_keys_bonding_keys[] = {
 	{
@@ -1120,6 +1121,23 @@ void bond_change_active_slave(struct bon
 			if (BOND_MODE(bond) == BOND_MODE_8023AD)
 				bond_3ad_handle_link_change(new_active, BOND_LINK_UP);
 
+			/* QCA NSS ECM support - Start */
+                        if (bond->params.mode == BOND_MODE_XOR) {
+                                struct bond_cb *lag_cb_main;
+
+                                rcu_read_lock();
+                                lag_cb_main = rcu_dereference(bond_cb);
+                                if (lag_cb_main &&
+                                    lag_cb_main->bond_cb_link_up) {
+                                        struct net_device *dev;
+
+                                        dev = new_active->dev;
+                                        lag_cb_main->bond_cb_link_up(dev);
+                                }
+                                rcu_read_unlock();
+                        }
+                        /* QCA NSS ECM support - End */
+
 			if (bond_is_lb(bond))
 				bond_alb_handle_link_change(bond, new_active, BOND_LINK_UP);
 		} else {
@@ -1697,6 +1715,7 @@ int bond_enslave(struct net_device *bond
 	const struct net_device_ops *slave_ops = slave_dev->netdev_ops;
 	struct slave *new_slave = NULL, *prev_slave;
 	struct sockaddr_storage ss;
+	struct bond_cb *lag_cb_main; /* QCA NSS ECM support */
 	int link_reporting;
 	int res = 0, i;
 
@@ -2099,6 +2118,13 @@ int bond_enslave(struct net_device *bond
 	if (bond_mode_can_use_xmit_hash(bond))
 		bond_update_slave_arr(bond, NULL);
 
+	/* QCA NSS ECM support - Start */
+        rcu_read_lock();
+        lag_cb_main = rcu_dereference(bond_cb);
+        if (lag_cb_main && lag_cb_main->bond_cb_enslave)
+                lag_cb_main->bond_cb_enslave(slave_dev);
+        rcu_read_unlock();
+        /* QCA NSS ECM support - End */
 
 	slave_info(bond_dev, slave_dev, "Enslaving as %s interface with %s link\n",
 		   bond_is_active_slave(new_slave) ? "an active" : "a backup",
@@ -2171,6 +2197,14 @@ err_undo_flags:
 		}
 	}
 
+	/* QCA NSS ECM support - Start */
+        rcu_read_lock();
+        lag_cb_main = rcu_dereference(bond_cb);
+        if (lag_cb_main && lag_cb_main->bond_cb_enslave)
+                lag_cb_main->bond_cb_enslave(slave_dev);
+        rcu_read_unlock();
+        /* QCA NSS ECM support - End */
+
 	return res;
 }
 
@@ -2192,6 +2226,7 @@ static int __bond_release_one(struct net
 	struct bonding *bond = netdev_priv(bond_dev);
 	struct slave *slave, *oldcurrent;
 	struct sockaddr_storage ss;
+	struct bond_cb *lag_cb_main; /* QCA NSS ECM support */
 	int old_flags = bond_dev->flags;
 	netdev_features_t old_features = bond_dev->features;
 
@@ -2214,6 +2249,14 @@ static int __bond_release_one(struct net
 
 	bond_set_slave_inactive_flags(slave, BOND_SLAVE_NOTIFY_NOW);
 
+	/* QCA NSS ECM support - Start */
+        rcu_read_lock();
+        lag_cb_main = rcu_dereference(bond_cb);
+        if (lag_cb_main && lag_cb_main->bond_cb_release)
+                lag_cb_main->bond_cb_release(slave_dev);
+        rcu_read_unlock();
+        /* QCA NSS ECM support - End */
+
 	bond_sysfs_slave_del(slave);
 
 	/* recompute stats just before removing the slave */
@@ -2512,6 +2555,11 @@ static void bond_miimon_commit(struct bo
 	struct list_head *iter;
 	struct slave *slave, *primary;
 
+	/* QCA NSS ECM support - Start */
+        struct net_device *slave_dev = NULL;
+        struct bond_cb *lag_cb_main;
+        /* QCA NSS ECM support - End */
+
 	bond_for_each_slave(bond, slave, iter) {
 		switch (slave->link_new_state) {
 		case BOND_LINK_NOCHANGE:
@@ -2554,6 +2602,12 @@ static void bond_miimon_commit(struct bo
 
 			bond_miimon_link_change(bond, slave, BOND_LINK_UP);
 
+			/* QCA NSS ECM support - Start */
+                        if ((bond->params.mode == BOND_MODE_XOR) &&
+                            (!slave_dev))
+                                slave_dev = slave->dev;
+                        /* QCA NSS ECM support - End */
+
 			if (!bond->curr_active_slave || slave == primary)
 				goto do_failover;
 
@@ -2595,6 +2649,15 @@ do_failover:
 	}
 
 	bond_set_carrier(bond);
+
+	/* QCA NSS ECM support - Start */
+        rcu_read_lock();
+        lag_cb_main = rcu_dereference(bond_cb);
+
+        if (slave_dev && lag_cb_main && lag_cb_main->bond_cb_link_up)
+                lag_cb_main->bond_cb_link_up(slave_dev);
+        rcu_read_unlock();
+        /* QCA NSS ECM support - End */
 }
 
 /* bond_mii_monitor
@@ -4822,6 +4885,11 @@ static void bond_destructor(struct net_d
 	struct bonding *bond = netdev_priv(bond_dev);
 	if (bond->wq)
 		destroy_workqueue(bond->wq);
+
+	/* QCA NSS ECM Support - Start */
+        if (bond->id != (~0U))
+                clear_bit(bond->id, &bond_id_mask);
+        /* QCA NSS ECM Support - End */
 }
 
 void bond_setup(struct net_device *bond_dev)
@@ -5390,6 +5458,16 @@ int bond_create(struct net *net, const c
 	bond_work_init_all(bond);
 
 	rtnl_unlock();
+
+	/* QCA NSS ECM Support - Start */
+        bond = netdev_priv(bond_dev);
+        bond->id = ~0U;
+        if (bond_id_mask != (~0UL)) {
+                bond->id = (u32)ffz(bond_id_mask);
+                set_bit(bond->id, &bond_id_mask);
+        }
+        /* QCA NSS ECM Support - End */
+
 	return 0;
 }
 
@@ -5487,6 +5565,204 @@ static void __exit bonding_exit(void)
 #endif
 }
 
+/* QCA NSS ECM support - Start */
+static bool bond_flow_dissect_without_skb(struct bonding *bond,
+                                          u8 *src_mac, u8 *dst_mac,
+                                          void *psrc, void *pdst,
+                                          u16 protocol, __be16 *layer4hdr,
+                                          struct flow_keys *fk)
+{
+        u32 *src = NULL;
+        u32 *dst = NULL;
+
+        fk->ports.ports = 0;
+        src = (uint32_t *)psrc;
+        dst = (uint32_t *)pdst;
+
+        if (protocol == htons(ETH_P_IP)) {
+                /* V4 addresses and address type*/
+                fk->addrs.v4addrs.src = src[0];
+                fk->addrs.v4addrs.dst = dst[0];
+                fk->control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;
+        } else if (protocol == htons(ETH_P_IPV6)) {
+                /* V6 addresses and address type*/
+                memcpy(&fk->addrs.v6addrs.src, src, sizeof(struct in6_addr));
+                memcpy(&fk->addrs.v6addrs.dst, dst, sizeof(struct in6_addr));
+                fk->control.addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;
+        } else {
+                return false;
+        }
+        if ((bond->params.xmit_policy == BOND_XMIT_POLICY_LAYER34) &&
+            (layer4hdr))
+                fk->ports.ports = *layer4hdr;
+
+        return true;
+}
+
+/* Extract the appropriate headers based on bond's xmit policy */
+
+/* bond_xmit_hash_without_skb - Applies load balancing algorithm for a packet,
+ * to calculate hash for a given set of L2/L3 addresses. Does not
+ * calculate egress interface.
+ */
+uint32_t bond_xmit_hash_without_skb(u8 *src_mac, u8 *dst_mac,
+                                    void *psrc, void *pdst, u16 protocol,
+                                    struct net_device *bond_dev,
+                                    __be16 *layer4hdr)
+{
+        struct bonding *bond = netdev_priv(bond_dev);
+        struct flow_keys flow;
+        u32 hash = 0;
+
+        if (bond->params.xmit_policy == BOND_XMIT_POLICY_LAYER2 ||
+            !bond_flow_dissect_without_skb(bond, src_mac, dst_mac, psrc,
+                                           pdst, protocol, layer4hdr, &flow))
+                return (dst_mac[5] ^ src_mac[5]);
+
+        if (bond->params.xmit_policy == BOND_XMIT_POLICY_LAYER23)
+                hash = dst_mac[5] ^ src_mac[5];
+        else if (layer4hdr)
+                hash = (__force u32)flow.ports.ports;
+
+        hash ^= (__force u32)flow_get_u32_dst(&flow) ^
+                (__force u32)flow_get_u32_src(&flow);
+        hash ^= (hash >> 16);
+        hash ^= (hash >> 8);
+
+        return hash;
+}
+
+/* bond_xor_get_tx_dev - Calculate egress interface for a given packet for a LAG
+ * that is configured in balance-xor mode
+ * @skb: pointer to skb to be egressed
+ * @src_mac: pointer to source L2 address
+ * @dst_mac: pointer to destination L2 address
+ * @src: pointer to source L3 address in network order
+ * @dst: pointer to destination L3 address in network order
+ * @protocol: L3 protocol
+ * @bond_dev: pointer to bond master device
+ *
+ * If @skb is NULL, bond_xmit_hash_without_skb is used to calculate hash using
+ * L2/L3 addresses.
+ *
+ * Returns: Either valid slave device, or NULL otherwise
+ */
+static struct net_device *bond_xor_get_tx_dev(struct sk_buff *skb,
+                                              u8 *src_mac, u8 *dst_mac,
+                                              void *src, void *dst,
+                                              u16 protocol,
+                                              struct net_device *bond_dev,
+                                              __be16 *layer4hdr)
+{
+        struct bonding *bond = netdev_priv(bond_dev);
+        int slave_cnt = READ_ONCE(bond->slave_cnt);
+        int slave_id = 0, i = 0;
+        u32 hash;
+        struct list_head *iter;
+        struct slave *slave;
+
+        if (slave_cnt == 0) {
+                pr_debug("%s: Error: No slave is attached to the interface\n",
+                         bond_dev->name);
+                return NULL;
+        }
+
+        if (skb) {
+                hash = bond_xmit_hash(bond, skb);
+                slave_id = hash % slave_cnt;
+        } else {
+                if (bond->params.xmit_policy != BOND_XMIT_POLICY_LAYER23 &&
+                    bond->params.xmit_policy != BOND_XMIT_POLICY_LAYER2 &&
+                    bond->params.xmit_policy != BOND_XMIT_POLICY_LAYER34) {
+                        pr_debug("%s: Error: Unsupported hash policy for balance-XOR fast path\n",
+                                 bond_dev->name);
+                        return NULL;
+		}
+
+                hash = bond_xmit_hash_without_skb(src_mac, dst_mac, src,
+                                                  dst, protocol, bond_dev,
+                                                  layer4hdr);
+                slave_id = hash % slave_cnt;
+        }
+
+        i = slave_id;
+
+        /* Here we start from the slave with slave_id */
+        bond_for_each_slave_rcu(bond, slave, iter) {
+                if (--i < 0) {
+                        if (bond_slave_can_tx(slave))
+                                return slave->dev;
+                }
+        }
+
+        /* Here we start from the first slave up to slave_id */
+        i = slave_id;
+        bond_for_each_slave_rcu(bond, slave, iter) {
+                if (--i < 0)
+                        break;
+                if (bond_slave_can_tx(slave))
+                        return slave->dev;
+        }
+
+        return NULL;
+}
+
+/* bond_get_tx_dev - Calculate egress interface for a given packet.
+ *
+ * Supports 802.3AD and balance-xor modes
+ *
+ * @skb: pointer to skb to be egressed, if valid
+ * @src_mac: pointer to source L2 address
+ * @dst_mac: pointer to destination L2 address
+ * @src: pointer to source L3 address in network order
+ * @dst: pointer to destination L3 address in network order
+ * @protocol: L3 protocol id from L2 header
+ * @bond_dev: pointer to bond master device
+ *
+ * Returns: Either valid slave device, or NULL for un-supported LAG modes
+ */
+struct net_device *bond_get_tx_dev(struct sk_buff *skb, uint8_t *src_mac,
+                                   u8 *dst_mac, void *src,
+                                   void *dst, u16 protocol,
+                                   struct net_device *bond_dev,
+                                   __be16 *layer4hdr)
+{
+        struct bonding *bond = netdev_priv(bond_dev);
+
+        if (!bond)
+                return NULL;
+
+        switch (bond->params.mode) {
+        case BOND_MODE_XOR:
+                return bond_xor_get_tx_dev(skb, src_mac, dst_mac,
+                                           src, dst, protocol,
+                                           bond_dev, layer4hdr);
+        case BOND_MODE_8023AD:
+                return bond_3ad_get_tx_dev(skb, src_mac, dst_mac,
+                                           src, dst, protocol,
+                                           bond_dev, layer4hdr);
+        default:
+                return NULL;
+        }
+}
+EXPORT_SYMBOL(bond_get_tx_dev);
+
+int bond_get_id(struct net_device *bond_dev)
+{
+        struct bonding *bond;
+
+        if (!((bond_dev->priv_flags & IFF_BONDING) &&
+              (bond_dev->flags & IFF_MASTER)))
+                return -EINVAL;
+
+        bond = netdev_priv(bond_dev);
+
+        return bond->id;
+}
+EXPORT_SYMBOL(bond_get_id);
+/* QCA NSS ECM support - End */
+
+
 module_init(bonding_init);
 module_exit(bonding_exit);
 MODULE_LICENSE("GPL");
--- a/include/uapi/linux/if_bonding.h
+++ b/include/uapi/linux/if_bonding.h
@@ -151,6 +151,24 @@ enum {
 };
 #define BOND_3AD_STAT_MAX (__BOND_3AD_STAT_MAX - 1)
 
+/* QCA NSS ECM support - Start */
+#ifdef __KERNEL__
+struct bond_cb {
+        void (*bond_cb_link_up)(struct net_device *slave);
+        void (*bond_cb_link_down)(struct net_device *slave);
+        void (*bond_cb_enslave)(struct net_device *slave);
+        void (*bond_cb_release)(struct net_device *slave);
+        void (*bond_cb_delete_by_slave)(struct net_device *slave);
+        void (*bond_cb_delete_by_mac)(uint8_t *mac_addr);
+};
+
+extern int bond_register_cb(struct bond_cb *cb);
+extern void bond_unregister_cb(void);
+extern int bond_get_id(struct net_device *bond_dev);
+#endif /* __KERNEL__ */
+/* QCA NSS ECM support - End */
+
+
 #endif /* _LINUX_IF_BONDING_H */
 
 /*
--- a/include/net/bonding.h
+++ b/include/net/bonding.h
@@ -256,6 +256,7 @@ struct bonding {
 	/* protecting ipsec_list */
 	spinlock_t ipsec_lock;
 #endif /* CONFIG_XFRM_OFFLOAD */
+	u32      id; /* QCA NSS ECM support */
 };
 
 #define bond_slave_get_rcu(dev) \
@@ -770,4 +771,13 @@ static inline netdev_tx_t bond_tx_drop(s
 	return NET_XMIT_DROP;
 }
 
+/* QCA NSS ECM support - Start */
+extern struct bond_cb __rcu *bond_cb;
+
+uint32_t bond_xmit_hash_without_skb(uint8_t *src_mac, uint8_t *dst_mac,
+                                    void *psrc, void *pdst, uint16_t protocol,
+                                    struct net_device *bond_dev,
+                                    __be16 *layer4hdr);
+/* QCA NSS ECM support - End */
+
 #endif /* _NET_BONDING_H */
--- a/drivers/net/bonding/bond_3ad.c
+++ b/drivers/net/bonding/bond_3ad.c
@@ -111,6 +111,40 @@ static void ad_marker_response_received(
 					struct port *port);
 static void ad_update_actor_keys(struct port *port, bool reset);
 
+/* QCA NSS ECM support - Start */
+struct bond_cb __rcu *bond_cb;
+
+int bond_register_cb(struct bond_cb *cb)
+{
+        struct bond_cb *lag_cb;
+
+        rcu_read_lock();
+        lag_cb = kzalloc(sizeof(*lag_cb), GFP_ATOMIC | __GFP_NOWARN);
+        if (!lag_cb) {
+                rcu_read_unlock();
+                return -1;
+        }
+
+        memcpy((void *)lag_cb, (void *)cb, sizeof(*cb));
+        rcu_assign_pointer(bond_cb, lag_cb);
+        rcu_read_unlock();
+        return 0;
+}
+EXPORT_SYMBOL(bond_register_cb);
+
+void bond_unregister_cb(void)
+{
+        struct bond_cb *lag_cb_main;
+
+        rcu_read_lock();
+        lag_cb_main = rcu_dereference(bond_cb);
+        kfree(lag_cb_main);
+        rcu_assign_pointer(bond_cb, NULL);
+        rcu_read_unlock();
+}
+EXPORT_SYMBOL(bond_unregister_cb);
+/* QCA NSS ECM support - End */
+
 
 /* ================= api to bonding and kernel code ================== */
 
@@ -988,6 +1022,29 @@ static void ad_mux_machine(struct port *
 				port->actor_oper_port_state |=
 				    LACP_STATE_SYNCHRONIZATION;
 			}
+
+			/* QCA NSS ECM support - Start */
+                        /* Send a notificaton about change in state of this
+                         * port. We only want to handle case where port moves
+                         * from AD_MUX_COLLECTING_DISTRIBUTING ->
+                         * AD_MUX_ATTACHED.
+                         */
+                        if (bond_slave_is_up(port->slave) &&
+                            (last_state == AD_MUX_COLLECTING_DISTRIBUTING)) {
+                                struct bond_cb *lag_cb_main;
+
+                                rcu_read_lock();
+                                lag_cb_main = rcu_dereference(bond_cb);
+                                if (lag_cb_main &&
+                                    lag_cb_main->bond_cb_link_down) {
+                                        struct net_device *dev;
+
+                                        dev = port->slave->dev;
+                                        lag_cb_main->bond_cb_link_down(dev);
+                                }
+                                rcu_read_unlock();
+                        }
+                        /* QCA NSS ECM support - End */
 			break;
 		case AD_MUX_COLLECTING_DISTRIBUTING:
 			if (!(port->sm_vars & AD_PORT_SELECTED) ||
@@ -1067,7 +1124,7 @@ static void ad_mux_machine(struct port *
  * If lacpdu arrived, stop previous timer (if exists) and set the next state as
  * CURRENT. If timer expired set the state machine in the proper state.
  * In other cases, this function checks if we need to switch to other state.
- */
+*/
 static void ad_rx_machine(struct lacpdu *lacpdu, struct port *port)
 {
 	rx_states_t last_state;
@@ -1885,6 +1942,8 @@ static void ad_enable_collecting_distrib
 					      bool *update_slave_arr)
 {
 	if (port->aggregator->is_active) {
+		struct bond_cb *lag_cb_main; /* QCA NSS ECM support */
+
 		slave_dbg(port->slave->bond->dev, port->slave->dev,
 			  "Enabling port %d (LAG %d)\n",
 			  port->actor_port_number,
@@ -1892,6 +1951,16 @@ static void ad_enable_collecting_distrib
 		__enable_port(port);
 		/* Slave array needs update */
 		*update_slave_arr = true;
+
+		/* QCA NSS ECM support - Start */
+                rcu_read_lock();
+                lag_cb_main = rcu_dereference(bond_cb);
+
+                if (lag_cb_main && lag_cb_main->bond_cb_link_up)
+                        lag_cb_main->bond_cb_link_up(port->slave->dev);
+
+                rcu_read_unlock();
+                /* QCA NSS ECM support - End */
 	}
 }
 
@@ -2750,3 +2819,102 @@ int bond_3ad_stats_fill(struct sk_buff *
 
 	return 0;
 }
+
+/* QCA NSS ECM support - Start */
+/* bond_3ad_get_tx_dev - Calculate egress interface for a given packet,
+ * for a LAG that is configured in 802.3AD mode
+ * @skb: pointer to skb to be egressed
+ * @src_mac: pointer to source L2 address
+ * @dst_mac: pointer to destination L2 address
+ * @src: pointer to source L3 address
+ * @dst: pointer to destination L3 address
+ * @protocol: L3 protocol id from L2 header
+ * @bond_dev: pointer to bond master device
+ *
+ * If @skb is NULL, bond_xmit_hash is used to calculate hash using L2/L3
+ * addresses.
+ *
+ * Returns: Either valid slave device, or NULL otherwise
+ */
+struct net_device *bond_3ad_get_tx_dev(struct sk_buff *skb, u8 *src_mac,
+                                       u8 *dst_mac, void *src,
+                                       void *dst, u16 protocol,
+                                       struct net_device *bond_dev,
+                                       __be16 *layer4hdr)
+{
+        struct bonding *bond = netdev_priv(bond_dev);
+        struct aggregator *agg;
+        struct ad_info ad_info;
+        struct list_head *iter;
+        struct slave *slave;
+        struct slave *first_ok_slave = NULL;
+        u32 hash = 0;
+        int slaves_in_agg;
+        int slave_agg_no = 0;
+        int agg_id;
+
+        if (__bond_3ad_get_active_agg_info(bond, &ad_info)) {
+                pr_debug("%s: Error: __bond_3ad_get_active_agg_info failed\n",
+                         bond_dev->name);
+                return NULL;
+        }
+
+        slaves_in_agg = ad_info.ports;
+        agg_id = ad_info.aggregator_id;
+
+        if (slaves_in_agg == 0) {
+                pr_debug("%s: Error: active aggregator is empty\n",
+                         bond_dev->name);
+                return NULL;
+	}
+
+	if (skb) {
+                hash = bond_xmit_hash(bond, skb);
+                slave_agg_no = hash % slaves_in_agg;
+        } else {
+                if (bond->params.xmit_policy != BOND_XMIT_POLICY_LAYER23 &&
+                    bond->params.xmit_policy != BOND_XMIT_POLICY_LAYER2 &&
+                    bond->params.xmit_policy != BOND_XMIT_POLICY_LAYER34) {
+                        pr_debug("%s: Error: Unsupported hash policy for 802.3AD fast path\n",
+                                 bond_dev->name);
+                        return NULL;
+                }
+
+                hash = bond_xmit_hash_without_skb(src_mac, dst_mac,
+                                                  src, dst, protocol,
+                                                  bond_dev, layer4hdr);
+                slave_agg_no = hash % slaves_in_agg;
+        }
+
+        bond_for_each_slave_rcu(bond, slave, iter) {
+                agg = SLAVE_AD_INFO(slave)->port.aggregator;
+                if (!agg || agg->aggregator_identifier != agg_id)
+                        continue;
+
+                if (slave_agg_no >= 0) {
+                        if (!first_ok_slave && bond_slave_can_tx(slave))
+                                first_ok_slave = slave;
+                        slave_agg_no--;
+                        continue;
+                }
+
+                if (bond_slave_can_tx(slave))
+                        return slave->dev;
+        }
+
+        if (slave_agg_no >= 0) {
+                pr_err("%s: Error: Couldn't find a slave to tx on for aggregator ID %d\n",
+                       bond_dev->name, agg_id);
+                return NULL;
+        }
+
+        /* we couldn't find any suitable slave after the agg_no, so use the
+         * first suitable found, if found.
+         */
+        if (first_ok_slave)
+                return first_ok_slave->dev;
+
+        return NULL;
+}
+/* QCA NSS ECM support - End */
+
--- a/include/net/bond_3ad.h
+++ b/include/net/bond_3ad.h
@@ -307,5 +307,13 @@ void bond_3ad_update_lacp_rate(struct bo
 void bond_3ad_update_ad_actor_settings(struct bonding *bond);
 int bond_3ad_stats_fill(struct sk_buff *skb, struct bond_3ad_stats *stats);
 size_t bond_3ad_stats_size(void);
+
+/* QCA NSS ECM support - Start */
+struct net_device *bond_3ad_get_tx_dev(struct sk_buff *skb, uint8_t *src_mac,
+                                       uint8_t *dst_mac, void *src,
+                                       void *dst, uint16_t protocol,
+                                       struct net_device *bond_dev,
+                                       __be16 *layer4hdr);
+/* QCA NSS ECM support - End */
 #endif /* _NET_BOND_3AD_H */
 
